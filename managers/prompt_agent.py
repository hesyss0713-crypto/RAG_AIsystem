import json
import re
from pathlib import Path
from managers.llm_manager import LLMManager
from managers.db_manager import get_connection
from psycopg2.extras import execute_values
from managers.chunker import CodeChunker
from managers.symbol import SymbolExtractor
from managers.embedding import EmbeddingManager

_shared_llm = None
_shared_emb = None


def get_llm_manager():
    global _shared_llm
    if _shared_llm is None:
        _shared_llm = LLMManager()
    return _shared_llm


def get_embedding_manager():
    global _shared_emb
    if _shared_emb is None:
        _shared_emb = EmbeddingManager()
    return _shared_emb


class LLMAgent:
    def __init__(self):
        self.llm = get_llm_manager()
        self.emb = get_embedding_manager()

    # -------------------------------------------------------------
    # ğŸ”¹ íŒŒì¼ ìš”ì•½
    # -------------------------------------------------------------
    def summarize_file(self, file_path: Path) -> str:
        """LLM ê¸°ë°˜ ì½”ë“œ/ë¬¸ì„œ ìš”ì•½"""
        ext = file_path.suffix.lower()

        # ë¹„ì½”ë“œ íŒŒì¼ ê³ ì • ë¬¸ì¥
        if ext in [".png", ".jpg", ".jpeg", ".gif"]:
            return "ì´ë¯¸ì§€ ë¦¬ì†ŒìŠ¤ íŒŒì¼ì…ë‹ˆë‹¤."
        if ext in [".npy", ".npz", ".pt", ".pkl", ".h5"]:
            return "ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ ë°ì´í„° ë˜ëŠ” ê°€ì¤‘ì¹˜ íŒŒì¼ì…ë‹ˆë‹¤."
        if ext in [".csv", ".xlsx"]:
            return "ë°ì´í„°ì…‹ íŒŒì¼ì…ë‹ˆë‹¤."

        try:
            text = file_path.read_text(encoding="utf-8", errors="ignore")[:4000]
        except Exception:
            return "íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        user_prompt = f"File name: {file_path.name}\n\nCode content:\n{text}"
        result = self.llm.generate(user_prompt, task="summarization", max_new_tokens=2048)
        if "<summary>" in result:
            result = result.split("<summary>")[-1].split("</summary>")[0]
        return result.strip()


    # -------------------------------------------------------------
    # ğŸ”¹ ì½”ë“œ semantic chunk ìƒì„±
    # -------------------------------------------------------------
    def safe_json_parse(self, raw: str):
        """LLM ì¶œë ¥ ë¬¸ìì—´ì„ ì•ˆì „í•˜ê²Œ JSONìœ¼ë¡œ ë³€í™˜ (ê¹¨ì§ ë³´ì • í¬í•¨)"""
        match = re.search(r"\[.*\]", raw, re.DOTALL)
        if not match:
            return []
        clean = match.group(0)

        # ê¸°ë³¸ ë¬¸ì ì •ë¦¬
        clean = clean.replace("â€™", "'").replace("â€œ", '"').replace("â€", '"')
        clean = re.sub(r"^```(json)?|```$", "", clean.strip(), flags=re.MULTILINE)

        # ğŸ§© content ë‚´ë¶€ì˜ " escape ì²˜ë¦¬
        def escape_quotes_in_content(m):
            content = m.group(1)
            # \ ë¨¼ì € escape â†’ " escape
            content = content.replace("\\", "\\\\").replace('"', '\\"')
            return f'"content": "{content}"'

        # "content": " ... " ë¶€ë¶„ì„ ì°¾ì•„ ë‚´ë¶€ ë”°ì˜´í‘œ ì´ìŠ¤ì¼€ì´í”„
        clean = re.sub(r'"content":\s*"(.*?)"', escape_quotes_in_content, clean, flags=re.DOTALL)

        # ê°ì²´ ê°„ ì‰¼í‘œ ëˆ„ë½ ë³´ì • (}{ â†’ },{)
        clean = re.sub(r'(?<=\})(\s*)(?=\{)', ', ', clean)

        # ë°°ì—´ ë˜ëŠ” ê°ì²´ ëì˜ íŠ¸ë ˆì¼ë§ ì½¤ë§ˆ ì œê±°
        clean = re.sub(r",\s*(\]|\})", r"\1", clean)

        try:
            return json.loads(clean)
        except json.JSONDecodeError as e:
            print(f"[Chunk] âš ï¸ Safe JSON decode error: {e}")
            print("---- raw json ----")
            print(clean[:])
            print("------------------")
            return []



    def extract_chunks(self, file_path: Path):
        chunker = CodeChunker()
        chunks = chunker.extract_chunks(file_path)
        if not chunks:
            print(f"[Chunk] âš ï¸ {file_path.name}: no chunks found")
            return []
        print(f"[Chunk] âœ… {file_path.name}: {len(chunks)} chunks parsed locally")
        return chunks
        
    # -------------------------------------------------------------
    # ğŸ”¹ symbol_links (AST + LLM hybrid)
    # -------------------------------------------------------------
    def extract_symbol_links(self, repo_id: int, repo_dir: Path):
        """AST + LLM hybrid ë°©ì‹ìœ¼ë¡œ symbol_links ì±„ìš°ê¸°"""
        from managers.symbol import SymbolExtractor  # ì´ë¯¸ ìƒë‹¨ importë˜ì–´ ìˆìœ¼ë©´ ìƒëµ ê°€ëŠ¥
        from managers.db_manager import get_connection
        from psycopg2.extras import execute_values

        extractor = SymbolExtractor(llm=self.llm)
        all_links = []

        for py_file in repo_dir.rglob("*.py"):
            try:
                links = extractor.extract_links(py_file, repo_id)
                if links:
                    all_links.extend(links)
            except Exception as e:
                print(f"[SymbolExtractor] âš ï¸ {py_file} skipped: {e}")

        if not all_links:
            print(f"[SymbolExtractor] âš ï¸ No symbol links found for repo_id={repo_id}")
            return

        conn = get_connection()
        cur = conn.cursor()
        execute_values(cur, """
            INSERT INTO symbol_links (repo_id, source_symbol, target_symbol, relation_type, file_path)
            VALUES %s
        """, [
            (l["repo_id"], l["source_symbol"], l["target_symbol"], l["relation_type"], l["file_path"])
            for l in all_links
        ])
        conn.commit()
        cur.close()
        conn.close()
        print(f"[SymbolExtractor] âœ… Inserted {len(all_links)} symbol links for repo_id={repo_id}")

    # -------------------------------------------------------------
    # ğŸ”¹ repo_id ê¸°ì¤€ìœ¼ë¡œ íŒŒì¼ ì „ì²´ ìš”ì•½
    # -------------------------------------------------------------
    def summarize_repo_files(self, repo_id: int, repo_dir: Path):
        conn = get_connection()
        cur = conn.cursor()
        cur.execute("SELECT id, file_path FROM files_meta WHERE repo_id = %s;", (repo_id,))
        files = cur.fetchall()

        collected_summaries = []

        for file_id, rel_path in files:
            fpath = repo_dir / rel_path
            if not fpath.exists():
                continue
            try:
                summary = self.summarize_file(fpath)
                files_emb = self.emb.embed_text(summary)
                cur.execute("UPDATE files_meta SET summary = %s, embedding = %s WHERE id = %s;", 
                (summary, files_emb.tolist(), file_id))

                if summary and len(summary.strip()) > 0:
                    collected_summaries.append({
                        "summary": summary
                    })

                print(f"[Summary] âœ… {rel_path}")
            except Exception as e:
                print(f"[Summary] âš ï¸ {rel_path}: {e}")
        all_summaries = "\n".join([s["summary"] for s in collected_summaries])
        print(f"all summay : \n{all_summaries}\n")

        repo_summ = self.llm.generate(all_summaries, task = "repo_summary", max_new_tokens=2048)
        print(f"repo summ : \n{repo_summ}\n")
        repo_summ_emb = self.emb.embed_text(repo_summ)

        cur.execute("""
            UPDATE repo_meta
            SET repo_summary = %s, summary_embedding = %s
            WHERE id = %s;
        """, (repo_summ, repo_summ_emb.tolist(), repo_id))
        conn.commit()
        cur.close()
        conn.close()
        print(f"[Summary] âœ… repo_id={repo_id} summaries complete")

    # -------------------------------------------------------------
    # ğŸ”¹ repo_id ê¸°ì¤€ìœ¼ë¡œ ì „ì²´ chunk ìƒì„± í›„ DB ì €ì¥
    # -------------------------------------------------------------

    def chunk_repo_files(self, repo_id: int, repo_dir: Path):
        conn = get_connection()
        cur = conn.cursor()

        cur.execute("""
            SELECT id, file_path, file_type
            FROM files_meta
            WHERE repo_id = %s;
        """, (repo_id,))
        files = cur.fetchall()

        embedder = self.emb  
        all_values = []
        total_chunks = 0

        for file_id, rel_path, file_type in files:
            path = repo_dir / rel_path
            if not path.exists() or file_type not in ["py", "js", "ts", "java", "cpp"]:
                continue

            chunks = self.extract_chunks(path)
            if not chunks:
                continue

            for c in chunks:
                emb = embedder.embed_text(c["content"])  # âœ… content ì„ë² ë”©
                all_values.append((
                    repo_id,
                    file_id,
                    str(path),
                    file_type,
                    c["semantic_scope"],
                    c["hierarchical_context"],
                    c["content"],
                    len(c["content"].split()),
                    emb.tolist(),  # âœ… vector(1024)
                ))

            total_chunks += len(chunks)
            print(f"[Chunk+Embed] âœ… {path.name}: {len(chunks)} chunks embedded")

        if all_values:
            execute_values(cur, """
                INSERT INTO repo_chunks
                (repo_id, file_id, file_path, file_type,
                semantic_scope, hierarchical_context, content, token_count, embedding)
                VALUES %s;
            """, all_values)
            print(f"[Chunk+Embed] ğŸš€ Inserted {len(all_values)} chunks (with embeddings) for repo_id={repo_id}")

            # âœ… repo_meta ì—…ë°ì´íŠ¸
            cur.execute("""
                UPDATE repo_meta
                SET total_chunks = %s
                WHERE id = %s;
            """, (total_chunks, repo_id))

        conn.commit()
        cur.close()
        conn.close()

